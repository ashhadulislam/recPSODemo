{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a597e46-a663-42c4-ad53-1408d338f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# PSO Related\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, threshold=None, left=None, right=None, info_gain=None, value=None,\n",
    "                particle=None,min_val=None,max_val=None,samples=None):\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        self.value = value\n",
    "        self.particle = particle\n",
    "        self.samples = samples\n",
    "        \n",
    "def gini_index(y):\n",
    "    y = y.reshape(-1)\n",
    "    total_samples = len(y)\n",
    "    classes = set(y)\n",
    "    gini = 0.0\n",
    "    \n",
    "    for c in classes:\n",
    "        proportion = sum([(1 if label == c else 0) for label in y]) / total_samples\n",
    "        gini += proportion * (1 - proportion)\n",
    "    \n",
    "    return gini\n",
    "        \n",
    "def entropy(y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "def information_gain(particle, X, y):\n",
    "    res = multiply_weight(X, particle[0:X.shape[1]])\n",
    "    threshold = particle[-1]\n",
    "    X_left, y_left, X_right, y_right = split(X, y, res, threshold)\n",
    "\n",
    "    parent_entropy = entropy(y)\n",
    "    left_entropy = entropy(y_left)\n",
    "    right_entropy = entropy(y_right)\n",
    "    n = len(y)\n",
    "    n_left, n_right = len(y_left), len(y_right)\n",
    "\n",
    "    child_entropy = (n_left / n) * left_entropy + (n_right / n) * right_entropy\n",
    "    return parent_entropy - child_entropy\n",
    "\n",
    "def information_gain_gini(parent_y, left_y, right_y):\n",
    "    parent_gini = gini_index(parent_y)\n",
    "    left_weight = len(left_y) / len(parent_y)\n",
    "    right_weight = len(right_y) / len(parent_y)\n",
    "    left_gini = gini_index(left_y)\n",
    "    right_gini = gini_index(right_y)\n",
    "    return parent_gini - (left_weight * left_gini + right_weight * right_gini)\n",
    "\n",
    "def objective_function(particle):\n",
    "    return -information_gain(particle, X, y)\n",
    "\n",
    "def information_gain_gini_given_particle(X, y, a_particle):\n",
    "    weights = a_particle[:-1]\n",
    "    threshold = a_particle[-1]\n",
    "    products = np.dot(X, weights)\n",
    "    products = (products - np.min(products)) / (np.max(products) - np.min(products))\n",
    "    X_left = X[np.where(products < threshold)]\n",
    "    X_right = X[np.where(products > threshold)]\n",
    "    y_left = y[np.where(products < threshold)]\n",
    "    y_right = y[np.where(products > threshold)]\n",
    "    return information_gain_gini(y, y_left, y_right)\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, dim):\n",
    "        self.position = np.random.rand(dim)\n",
    "        self.velocity = np.random.rand(dim)\n",
    "        self.best_position = self.position.copy()\n",
    "        self.best_fitness = float('-inf')\n",
    "\n",
    "def apply_PSO(num_particles, num_epochs, X, y, inertia_weight=0.25, cognitive_weight=0.5, social_weight=0.5, mask=None):\n",
    "    if not isinstance(mask, np.ndarray):                \n",
    "        mask = np.ones(X.shape[1])\n",
    "\n",
    "    dim = X.shape[1] + 1\n",
    "    swarm = [Particle(dim) for _ in range(num_particles)]\n",
    "\n",
    "    global_best_position = Particle(dim).position\n",
    "    global_best_fitness = float('-inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for particle in swarm:\n",
    "            fitness = information_gain_gini_given_particle(X, y, particle.position)\n",
    "            if fitness > global_best_fitness:\n",
    "                global_best_position = particle.position.copy()\n",
    "                global_best_fitness = fitness\n",
    "            if fitness > particle.best_fitness:\n",
    "                particle.best_position = particle.position.copy()\n",
    "                particle.best_fitness = fitness\n",
    "\n",
    "        for particle in swarm:\n",
    "            r1 = np.random.rand(dim)\n",
    "            r2 = np.random.rand(dim)\n",
    "\n",
    "            particle.velocity = (inertia_weight * particle.velocity +\n",
    "                                 cognitive_weight * r1 * (particle.best_position - particle.position) +\n",
    "                                 social_weight * r2 * (global_best_position - particle.position))\n",
    "            particle.position += particle.velocity\n",
    "            particle.position[:-1] = particle.position[:-1] * mask\n",
    "\n",
    "    return global_best_position, global_best_fitness\n",
    "\n",
    "def split(X, y, positions):\n",
    "    weights = positions[:-1]\n",
    "    threshold = positions[-1]\n",
    "    products = np.dot(X, weights)\n",
    "    \n",
    "    min_products = np.min(products)\n",
    "    max_products = np.max(products)\n",
    "    products = (products - min_products) / (max_products - min_products)\n",
    "    \n",
    "    X_left = X[np.where(products < threshold)]\n",
    "    X_right = X[np.where(products >= threshold)]\n",
    "    y_left = y[np.where(products < threshold)]\n",
    "    y_right = y[np.where(products >= threshold)]\n",
    "    \n",
    "    return X_left, y_left, X_right, y_right, min_products, max_products\n",
    "\n",
    "def calculate_leaf_value(Y):\n",
    "    Y = list(Y)\n",
    "    return max(Y, key=Y.count)\n",
    "\n",
    "def build_tree(X, y, curr_depth=0, max_depth=5, max_num_particles=10, max_num_epochs=10, min_split_size=3, mask=None):\n",
    "    num_particles = max(max_num_particles * (curr_depth + 1), 10)\n",
    "    num_epochs = max(max_num_epochs * (curr_depth + 1), 10)\n",
    "    \n",
    "    cntr = Counter(y)\n",
    "    nm = len(list(cntr.keys()))\n",
    "    \n",
    "    if curr_depth < max_depth and X.shape[0] > min_split_size and nm != 1:  \n",
    "        best_position, best_fitness = apply_PSO(num_particles, num_epochs, X, y, mask=mask)\n",
    "        X_left, y_left, X_right, y_right, min_products, max_products = split(X, y, best_position)\n",
    "        print(\"depth\",curr_depth,X.shape,\"next\",curr_depth+1,X_left.shape,X_right.shape)\n",
    "        print(\"\\n\",Counter(list(y.reshape(-1))),\"\\n\",Counter(list(y_left.reshape(-1))),\"\\n\",Counter(list(y_right.reshape(-1))))\n",
    "        \n",
    "        if X_left.shape[0] == 0 or X_right.shape[0] == 0:\n",
    "            leaf_value = calculate_leaf_value(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        left_subtree = build_tree(X_left, y_left, curr_depth=curr_depth+1, max_depth=max_depth,\n",
    "                                  max_num_particles=max_num_particles, max_num_epochs=max_num_epochs,\n",
    "                                  min_split_size=min_split_size, mask=mask)\n",
    "        right_subtree = build_tree(X_right, y_right, curr_depth=curr_depth+1, max_depth=max_depth,\n",
    "                                   max_num_particles=max_num_particles, max_num_epochs=max_num_epochs,\n",
    "                                   min_split_size=min_split_size, mask=mask)\n",
    "        return Node(threshold=best_position[-1], left=left_subtree, right=right_subtree,\n",
    "                    info_gain=best_fitness, particle=best_position, min_val=min_products, max_val=max_products,\n",
    "                    samples=X.shape[0])\n",
    "    \n",
    "    leaf_value = calculate_leaf_value(y)\n",
    "    return Node(value=leaf_value)\n",
    "\n",
    "def make_prediction(x, tree):\n",
    "    if tree.value is not None: return tree.value\n",
    "\n",
    "    particle = tree.particle\n",
    "    weights = particle[:-1]\n",
    "    res = np.dot(x, weights)\n",
    "\n",
    "    min_val = tree.min_val\n",
    "    max_val = tree.max_val\n",
    "    res = (res - min_val) / (max_val - min_val)\n",
    "    if res < tree.threshold:\n",
    "        return make_prediction(x, tree.left)\n",
    "    else:\n",
    "        return make_prediction(x, tree.right)\n",
    "\n",
    "def predict(X, root):\n",
    "    predictions = [make_prediction(x, root) for x in X]\n",
    "    return predictions\n",
    "\n",
    "def score(predicted_labels, y):\n",
    "    correct = sum(1 for i in range(len(y)) if predicted_labels[i] == y[i])\n",
    "    return correct / len(y)\n",
    "\n",
    "def traverse_get_weights_samples(root):\n",
    "    if not root:\n",
    "        return []\n",
    "    stack, weights, populations = [(root, 0)], [], []\n",
    "    \n",
    "    while stack:\n",
    "        node, level = stack.pop()\n",
    "        if node:\n",
    "            if isinstance(node.particle, np.ndarray):                \n",
    "                weights.append(node.particle[:-1])\n",
    "                populations.append(node.samples)\n",
    "            stack.append((node.right, level + 1))\n",
    "            stack.append((node.left, level + 1))\n",
    "    return weights, populations\n",
    "\n",
    "def calculate_weighted_average_particles(weights, populations, prune_rate=1.2):\n",
    "    total = 0\n",
    "    vals = np.zeros(weights[0].shape[0])\n",
    "    for i in range(len(weights)):\n",
    "        vals += np.array(weights[i]) * populations[i]\n",
    "        total += populations[i]\n",
    "    vals = vals / total\n",
    "    vals = (vals - np.min(vals)) / (np.max(vals) - np.min(vals))\n",
    "    thresh = prune_rate * np.std(vals)\n",
    "    vals[vals < thresh] = 0\n",
    "    vals[vals >= thresh] = 1\n",
    "    return vals\n",
    "\n",
    "def calculate_feat_importance(weights, populations):\n",
    "    total = 0\n",
    "    vals = np.zeros(weights[0].shape[0])\n",
    "    for i in range(len(weights)):\n",
    "        vals += np.array(weights[i]) * populations[i]\n",
    "        total += populations[i]\n",
    "    vals = vals / total\n",
    "    vals = (vals - np.min(vals)) / (np.max(vals) - np.min(vals))    \n",
    "    return vals    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd13def-b318-4f30-8973-93a036f0b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "depth = 6\n",
    "max_num_epochs = 100\n",
    "max_num_particles = 100\n",
    "min_split_size = 5\n",
    "prune_rate = 1.0\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('/Users/dr.ashhadulislam/projects/postDoc_HBKU/new_ideas/Explainability/nip2022_interpretable_dts/interpretable-dts/data/audit_data/audit_risk_formatted.csv')\n",
    "# data = pd.read_csv('alger_forest_fire.csv')\n",
    "print(\"data shape\", data.shape)\n",
    "print(data.columns)\n",
    "\n",
    "# Assuming the last column is the label\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "X=np.nan_to_num(X)\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5eb128-d27a-4796-bf24-705c6cd4824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe683c-1d97-45e5-b489-ff3b364d1148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate the initial tree\n",
    "tree = build_tree(X_train, y_train, max_depth=depth, max_num_epochs=max_num_epochs * 2, max_num_particles=max_num_particles * 2, min_split_size=min_split_size)\n",
    "y_pred = predict(X_test, tree)\n",
    "pre_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Pre-Pruning Accuracy: {pre_acc:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9580c8-1811-414f-a250-73c52ea6281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning\n",
    "weights, populations = traverse_get_weights_samples(tree)\n",
    "mask = calculate_weighted_average_particles(weights, populations, prune_rate=prune_rate)\n",
    "compression = len(np.where(mask == 0)[0]) / mask.size\n",
    "\n",
    "if compression >= 1:\n",
    "    print(f\"Prune rate {prune_rate} is too high. Consider selecting a lower prune rate.\")\n",
    "else:\n",
    "    tree_pruned = build_tree(X_train, y_train, max_depth=depth, max_num_epochs=max_num_epochs, max_num_particles=max_num_particles, min_split_size=min_split_size, mask=mask)\n",
    "    weights_pruned, populations = traverse_get_weights_samples(tree_pruned)\n",
    "    feat_importance = calculate_feat_importance(weights_pruned, populations)\n",
    "    feat_importance = feat_importance * mask\n",
    "\n",
    "    # Mapping weights to column names\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': data.columns[:-1],\n",
    "        'Importance': feat_importance\n",
    "    })\n",
    "\n",
    "    print(importance_df.head())\n",
    "\n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plotting the feature importance\n",
    "    print(\"Feature Importance After Pruning\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df, ax=ax)\n",
    "    ax.set_title(\"Feature Importance\")\n",
    "    plt.show()\n",
    "\n",
    "    y_pred_pruned = predict(X_test, tree_pruned)\n",
    "    post_acc = accuracy_score(y_test, y_pred_pruned)\n",
    "\n",
    "    print(f\"Post-Pruning Accuracy: {post_acc:.2f}\")\n",
    "\n",
    "    # Precision, Recall, F1 Score\n",
    "    precision = precision_score(y_test, y_pred_pruned, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_pred_pruned, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_pred_pruned, average=\"weighted\")\n",
    "\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_pruned)\n",
    "    print(\"Confusion Matrix\")\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310k",
   "language": "python",
   "name": "python310k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
